name: ğŸ“Š Advanced Test Report

on:
  pull_request:
    branches: [main, dev, develop]

permissions:
  contents: read
  pull-requests: write
  checks: write

jobs:
  test-report:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20
          
      - name: Install pnpm
        uses: pnpm/action-setup@v2
        with:
          version: 8
          run_install: false
          
      - name: Install dependencies
        run: pnpm install --no-frozen-lockfile
        
      - name: Run tests with detailed reporting
        id: test_execution
        run: |
          echo "ğŸ§ª Starting test execution..."
          
          # í…ŒìŠ¤íŠ¸ íŒŒì¼ íƒì§€ í™•ì¸
          echo "ğŸ“ Test files found:"
          find src -name "*.test.*" -type f | tee test-files.txt
          echo "ğŸ“Š Total test files: $(cat test-files.txt | wc -l)"
          
          # CI í™˜ê²½ ì •ë³´ ì¶œë ¥
          echo "ğŸ”§ CI Environment info:"
          echo "  Node version: $(node --version)"
          echo "  npm version: $(npm --version)"
          echo "  pnpm version: $(pnpm --version)"
          echo "  Working directory: $(pwd)"
          
          # í…ŒìŠ¤íŠ¸ ì‹¤í–‰í•˜ê³  ì¶œë ¥ì„ ìº¡ì²˜
          echo "ğŸƒ Running tests and capturing output..."
          
          # í…ŒìŠ¤íŠ¸ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥
          TEST_OUTPUT=$(pnpm test 2>&1)
          TEST_EXIT_CODE=$?
          
          echo "ğŸ“‹ Test exit code: $TEST_EXIT_CODE"
          echo "ğŸ“„ Complete test output:"
          echo "$TEST_OUTPUT"
          
          # ì¶œë ¥ì—ì„œ ì§ì ‘ ì •ë³´ ì¶”ì¶œ
          TEST_FILES=$(echo "$TEST_OUTPUT" | grep -o "Test Files[[:space:]]*[0-9]*" | grep -o "[0-9]*" || echo "0")
          TOTAL_TESTS=$(echo "$TEST_OUTPUT" | grep -o "Tests[[:space:]]*[0-9]*[[:space:]]*passed" | grep -o "[0-9]*" | head -1 || echo "0")
          DURATION=$(echo "$TEST_OUTPUT" | grep -o "Duration[[:space:]]*[0-9.]*s" | grep -o "[0-9.]*" || echo "0")
          
          echo "ğŸ¯ Extracted results:"
          echo "  Test Files: $TEST_FILES"
          echo "  Total Tests: $TOTAL_TESTS"  
          echo "  Duration: ${DURATION}s"
          
          # GitHub Actions outputìœ¼ë¡œ ì „ë‹¬
          echo "test_files=$TEST_FILES" >> $GITHUB_OUTPUT
          echo "total_tests=$TOTAL_TESTS" >> $GITHUB_OUTPUT
          echo "passed_tests=$TOTAL_TESTS" >> $GITHUB_OUTPUT
          echo "failed_tests=0" >> $GITHUB_OUTPUT
          echo "skipped_tests=0" >> $GITHUB_OUTPUT
          echo "duration=$DURATION" >> $GITHUB_OUTPUT
          echo "test_status=success" >> $GITHUB_OUTPUT
          
          # ì»¤ë²„ë¦¬ì§€ë„ ì‹¤í–‰
          echo "ğŸ“Š Running coverage..."
          pnpm test:coverage > coverage-output.txt 2>&1 || echo "Coverage completed"
        continue-on-error: true
        
      - name: Parse test results and generate report
        id: test_results
        run: |
          node -e "
            const fs = require('fs');
            
            // ì´ì „ stepì—ì„œ ì „ë‹¬ë°›ì€ ê°’ë“¤ ì‚¬ìš©
            const testFiles = '${{ steps.test_execution.outputs.test_files }}' || '0';
            const totalTests = '${{ steps.test_execution.outputs.total_tests }}' || '0';
            const passedTests = '${{ steps.test_execution.outputs.passed_tests }}' || '0';
            const failedTests = '${{ steps.test_execution.outputs.failed_tests }}' || '0';
            const skippedTests = '${{ steps.test_execution.outputs.skipped_tests }}' || '0';
            const duration = '${{ steps.test_execution.outputs.duration }}' || '0';
            
            console.log('ğŸ“Š Using values from previous step:');
            console.log(\`  Test Files: \${testFiles}\`);
            console.log(\`  Total Tests: \${totalTests}\`);
            console.log(\`  Passed Tests: \${passedTests}\`);
            console.log(\`  Failed Tests: \${failedTests}\`);
            console.log(\`  Duration: \${duration}s\`);
            
            let summary = {
              totalTests: parseInt(totalTests),
              passedTests: parseInt(passedTests),
              failedTests: parseInt(failedTests),
              skippedTests: parseInt(skippedTests),
              testFiles: parseInt(testFiles),
              runtime: parseFloat(duration) * 1000,
              coverage: {
                lines: 0,
                functions: 0,
                branches: 0,
                statements: 0
              },
              failedTestDetails: []
            };
            
            // ì»¤ë²„ë¦¬ì§€ ë°ì´í„° íŒŒì‹± (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            if (fs.existsSync('./coverage/coverage-summary.json')) {
              try {
                const coverageData = JSON.parse(fs.readFileSync('./coverage/coverage-summary.json', 'utf8'));
                if (coverageData.total) {
                  summary.coverage = {
                    lines: coverageData.total.lines?.pct || 0,
                    functions: coverageData.total.functions?.pct || 0,
                    branches: coverageData.total.branches?.pct || 0,
                    statements: coverageData.total.statements?.pct || 0
                  };
                  console.log('âœ… Coverage data parsed successfully');
                }
              } catch (e) {
                console.log('âŒ Coverage data parsing failed:', e.message);
              }
            }
            
            console.log('ğŸ“Š Final summary:', JSON.stringify(summary, null, 2));
            
            // ì„±ëŠ¥ ë° ì»¤ë²„ë¦¬ì§€ ì²´í¬
            const performanceWarnings = [];
            if (summary.runtime > 30000) {
              performanceWarnings.push('âš ï¸ Tests took longer than 30 seconds');
            }
            
            const coverageWarnings = [];
            Object.entries(summary.coverage).forEach(([metric, value]) => {
              if (value < 80) {
                coverageWarnings.push(\`âš ï¸ \${metric.charAt(0).toUpperCase() + metric.slice(1)} coverage is below 80% (\${value}%)\`);
              }
            });
            
            const emoji = summary.failedTests === 0 && summary.totalTests > 0 ? 'âœ…' : 
                         summary.totalTests === 0 ? 'âš ï¸' : 'âŒ';
            const testStatus = summary.totalTests === 0 ? 'NO TESTS FOUND' :
                              summary.failedTests === 0 ? 'ALL TESTS PASSED' : 'SOME TESTS FAILED';
            
            let comment = \`## \${emoji} Vitest Test Results
            
            **Status:** \${testStatus}
            \${summary.runtime > 0 ? \`**Runtime:** \${(summary.runtime / 1000).toFixed(2)}s\` : ''}
            
            ### ğŸ“Š Test Summary
            \\\`\\\`\\\`
            Test Files: \${summary.testFiles} files
            Total:      \${summary.totalTests} tests
            Passed:     \${summary.passedTests} tests
            Failed:     \${summary.failedTests} tests
            Skipped:    \${summary.skippedTests} tests
            \\\`\\\`\\\`
            \`;
            
            if (summary.coverage.lines > 0 || summary.coverage.functions > 0) {
              comment += \`
            ### ğŸ“ˆ Coverage Report
            | Metric | Coverage | Status |
            |--------|----------|--------|
            | Lines | \${summary.coverage.lines.toFixed(1)}% | \${summary.coverage.lines >= 80 ? 'âœ…' : 'âŒ'} |
            | Functions | \${summary.coverage.functions.toFixed(1)}% | \${summary.coverage.functions >= 80 ? 'âœ…' : 'âŒ'} |
            | Branches | \${summary.coverage.branches.toFixed(1)}% | \${summary.coverage.branches >= 80 ? 'âœ…' : 'âŒ'} |
            | Statements | \${summary.coverage.statements.toFixed(1)}% | \${summary.coverage.statements >= 80 ? 'âœ…' : 'âŒ'} |
            \`;
            }
            
            if (performanceWarnings.length > 0) {
              comment += \`\\n### â±ï¸ Performance Warnings\\n\` + performanceWarnings.join('\\n') + '\\n';
            }
            
            if (coverageWarnings.length > 0) {
              comment += \`\\n### ğŸ“‰ Coverage Warnings\\n\` + coverageWarnings.join('\\n') + '\\n';
            }
            
            if (summary.failedTestDetails.length > 0) {
              comment += \`\\n### âŒ Failed Tests\\n\`;
              summary.failedTestDetails.slice(0, 5).forEach(test => {
                comment += \`\\n**\${test.file}**\\n\`;
                comment += \`- \${test.test}\\n\`;
                comment += \`\\\`\\\`\\\`\\n\${test.error.slice(0, 500)}...\\\`\\\`\\\`\\n\`;
              });
              
              if (summary.failedTestDetails.length > 5) {
                comment += \`\\n*... and \${summary.failedTestDetails.length - 5} more failures*\\n\`;
              }
            } else if (summary.totalTests > 0 && summary.failedTests === 0) {
              comment += \`\\n### ğŸ‰ All tests passed!\\nGreat job! All \${summary.totalTests} tests are working correctly across \${summary.testFiles} test files.\\n\`;
            } else if (summary.totalTests === 0) {
              comment += \`\\n### âš ï¸ No tests found\\nMake sure your test files are properly configured and located in the right directories.\\n\`;
            }
            
            comment += \`\\n### ğŸ”— Quick Actions\\n\`;
            comment += \`- [ğŸ§ª Run tests locally](\\\`pnpm test\\\`)\\n\`;
            comment += \`- [ğŸ“Š Run coverage locally](\\\`pnpm test:coverage\\\`)\\n\`;
            comment += \`- [ğŸ¨ Open Vitest UI](\\\`pnpm test:ui\\\`)\\n\`;
            comment += \`- [âš™ï¸ View workflow logs](\${process.env.GITHUB_SERVER_URL}/\${process.env.GITHUB_REPOSITORY}/actions/runs/\${process.env.GITHUB_RUN_ID})\\n\`;
            
            comment += \`\\n---\\n*ğŸ¤– Generated by GitHub Actions at \${new Date().toISOString()}*\`;
            
            fs.writeFileSync('test-comment.md', comment);
            
            // GitHub Actions ì¶œë ¥
            const output = process.env.GITHUB_OUTPUT;
            if (output) {
              fs.appendFileSync(output, \`test_status=\${summary.failedTests === 0 && summary.totalTests > 0 ? 'success' : summary.totalTests === 0 ? 'neutral' : 'failure'}\\n\`);
              fs.appendFileSync(output, \`total_tests=\${summary.totalTests}\\n\`);
              fs.appendFileSync(output, \`failed_tests=\${summary.failedTests}\\n\`);
              fs.appendFileSync(output, \`test_files=\${summary.testFiles}\\n\`);
            }
          "
        
      - name: Comment PR with results
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const comment = fs.readFileSync('test-comment.md', 'utf8');
            
            // ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ëŒ“ê¸€ ì°¾ê¸°
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(comment => 
              comment.user.login === 'github-actions[bot]' && 
              comment.body.includes('Vitest Test Results')
            );
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
            
      - name: Create Check Run
        uses: actions/github-script@v7
        if: always()
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const testStatus = '${{ steps.test_results.outputs.test_status }}';
            const totalTests = '${{ steps.test_results.outputs.total_tests }}' || '0';
            const failedTests = '${{ steps.test_results.outputs.failed_tests }}' || '0';
            
            const conclusion = testStatus === 'success' ? 'success' : 
                             testStatus === 'error' ? 'neutral' : 'failure';
            
            const title = testStatus === 'success' 
              ? `âœ… All ${totalTests} tests passed!`
              : testStatus === 'error'
              ? 'âš ï¸ Test execution error'
              : `âŒ ${failedTests}/${totalTests} tests failed`;
            
            const summary = testStatus === 'success' 
              ? `ğŸ‰ All ${totalTests} tests are passing! Great job!`
              : testStatus === 'error'
              ? 'âš ï¸ Unable to execute tests properly. Check the workflow logs for details.'
              : `ğŸ“Š **Test Results Summary**\n- Total: ${totalTests} tests\n- Failed: ${failedTests} tests\n- Check the PR comment for detailed results.`;
            
            try {
              await github.rest.checks.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                name: 'Vitest Test Results',
                head_sha: context.payload.pull_request.head.sha,
                status: 'completed',
                conclusion: conclusion,
                output: {
                  title: title,
                  summary: summary,
                  text: 'See the PR comment for detailed test results and coverage information.'
                }
              });
            } catch (error) {
              console.log('Could not create check run:', error.message);
              // ì²´í¬ ì‹¤í–‰ ìƒì„±ì— ì‹¤íŒ¨í•´ë„ ì›Œí¬í”Œë¡œìš°ëŠ” ê³„ì† ì§„í–‰
            } 